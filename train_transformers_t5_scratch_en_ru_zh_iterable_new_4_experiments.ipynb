{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d627b974-2c6f-4235-a136-ad698230a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41908918-ed7c-42c8-ac67-1da733b3a128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0463307d-c10c-4252-8f4d-216e02a29291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, T5Tokenizer, T5TokenizerFast, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdc511f-2f1c-457a-a6ba-3b5296d120cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import kenlm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea9a31e-17dd-4dcd-808a-a556a10ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6859c4-24b7-4e6d-ba0b-19c866af915b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#root_dir = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf7fce1-399b-40a0-af29-32de84b7e77b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joefox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7860cbc5-87a0-4193-840e-23d4a6da23dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etap = 6 0.02\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "etap_train = 6\n",
    "\n",
    "if etap_train == 6:\n",
    "    batch_size = 16\n",
    "    gradient_accumulation_steps = 64\n",
    "    max_input_length = 200\n",
    "    max_target_length = 200\n",
    "\n",
    "    train_steps = 14187 #Устанавливается на втором шаге\n",
    "    num_train_epochs = 1.1\n",
    "    eval_steps = 1000\n",
    "    save_steps = 1000\n",
    "    warmup_steps = 2000\n",
    "    save_total_limit = 10\n",
    "    learning_rate=2e-2\n",
    "    learning_rate_final_cosine=1e-5\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "optim_weight_decay = 0.0\n",
    "\n",
    "print('etap =',etap_train,learning_rate)\n",
    "\n",
    "streaming = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16409b00-f8c4-4390-88bf-ccab8a98f8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")\n",
    "sacrebleu_score = evaluate.load(\"sacrebleu\")\n",
    "#meteor_score = evaluate.load('meteor')\n",
    "bertscore_ru = evaluate.load(\"bertscore\")\n",
    "bertscore_en = evaluate.load(\"bertscore\")\n",
    "bertscore_zh = evaluate.load(\"bertscore\")\n",
    "metric_chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc8824a-442d-4062-afa9-76d73b16076b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kenlm', 'models', 'tokenizers']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4388205-eacf-47ad-9097-d1f268900de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_model_ru = kenlm.Model(f'/data/kenlm/' + f'ru.arpa.bin')\n",
    "pp_model_en = kenlm.Model(f'/data/kenlm/' + f'en.arpa.bin')\n",
    "pp_model_zh = kenlm.Model(f'/data/kenlm/' + f'zh.arpa.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d1d43b-ecf3-48a5-bfd4-7893816c06a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_ru_zh_translate_corpus_freq_1  en_ru_zh_translate_corpus_micro\n"
     ]
    }
   ],
   "source": [
    "!ls /share/translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c19092-9fd1-46c9-9515-ba102f46c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.read_data_fast import FixLenFastReadDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17373862-1bc6-42e7-b5a8-48c330ef5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace__len__(a):\n",
    "#     return 32*30\n",
    "\n",
    "# FixLenFastReadDS.__len__ = replace__len__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768aff66-d1b3-4614-a0cd-5bd83d12d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/share/datasets_bin/en_ru_zh_translate_corpus_freq_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd59079e-8e16-43ee-8c9e-6961e7b76582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('/data/tokenizers/tokenizer_t5_en_ru_zh_65000', max_len=max_input_length)\n",
    "if etap_train == 6:\n",
    "    model_name = '/data/models/t5_scratch_translate_en_ru_zh_2023_10_16_20-36-18'\n",
    "    #model = T5ForConditionalGeneration.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688d782e-67e8-4cd4-8200-2951a98300d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d521b7-de7e-4ae4-8740-cb4fd41d6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if etap_train == 4:\n",
    "#     #Заморозим все слои кроме эмбедингов\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False \n",
    "#     for param in model.shared.parameters():\n",
    "#         param.requires_grad = True \n",
    "#     for param in model.decoder.embed_tokens.parameters():\n",
    "#         param.requires_grad = True \n",
    "#     for param in model.encoder.embed_tokens.parameters():\n",
    "#         param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96464830-7007-4f2b-9216-d6db696308e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, "
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad,end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b13b4119-2e13-45c6-b243-8dfdc125a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d713bc69a2942d88bae2309269341d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fccdbbb2efb43c1bdc7a318f220a88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rd_val = FixLenFastReadDS(data_dir,'val',max_input_length,tokenizer)\n",
    "rd_train = FixLenFastReadDS(data_dir,'val',max_input_length,tokenizer)\n",
    "#rd_train = FixLenFastReadDS(data_dir,'train',max_input_length,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f06abc-08fe-4554-845a-97b543132959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11842, 11842)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rd_val), len(rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c4749fb-6f33-4893-8d1b-206934c31b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rd_val[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f298ae5-b971-495e-9053-762adeacd852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('translate to ru: I believe that is only in navigable waters? :: And like Tempels, Kagame believes that the ancient monument of Bantu natural language contains markers of ontological commitments that are shared among all speakers of these languages, including commitment to the reality of God.. The NDRC has promoted more than 1,200 projects to private investors, covering a wide range of areas including transportation, energy and environmental protection, with total investment surpassing 2.5 trillion yuan (about 360 billion U.S. dollars). :: In all kinds of analysis, all processes should be regarded as links in a continuous chain of events, based on the laws of classical physics (experimentally verified) that do not contradict each other.. It confers resistance to light, exposed to the sun, temperature changes and high resistance to daily use furniture.</s>',\n",
       " 'В достатке он находится лишь в морских водах. :: Как и Темпелс, Кагаме считает, что древний памятник естественного языка банту содержит маркеры онтологических обязательств, которые разделяют все носители этих языков, в том числе приверженность реальности Бога.. Комитет представил частным инвесторам более 1200 проектов в таких сферах, как транспорт, энергетика и охрана экологии, с общим объемом капитала в размере свыше 2,5 трлн юаней /около 360 млрд долларов США/. :: В ходе всех анализов все процессы должны рассматриваться как звенья непрерывной цепочки событий, основанной на законах классической физики (экспериментально проверенных) не противоречащих друг другу.. Придает устойчивость к свету, воздействию солнца, изменению температуры, высокую устойчивость к ежедневному использованию мебели.</s>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_input, np_label = rd_val[0]['input_ids'], rd_val[0]['labels']\n",
    "tokenizer.decode(np_input), tokenizer.decode(np_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5635ebdd-4b61-4f9a-9d4d-2417ef5ef9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заморозим все слои кроме эмбедингов\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False \n",
    "# for param in model.shared.parameters():\n",
    "#     param.requires_grad = False \n",
    "# for param in model.decoder.embed_tokens.parameters():\n",
    "#     param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10e7c85f-5667-44b6-a9be-8777ded6d640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def current_time_in_second(): \n",
    "    #Возвращает время в секундах\n",
    "    return round(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca872fc4-0eae-4f2e-a507-bcd8efbd44ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_in_second_to_textdate(time_in_second):  \n",
    "    #Преобразовывает секунды в текстовую дату\n",
    "    local_time = time.localtime(time_in_second)\n",
    "    str_time = time.strftime(\"%Y_%m_%d_%H-%M-%S\", local_time)\n",
    "    return str_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df40419a-8e31-4dfe-a32c-a2acaceadd51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5_scratch_translate_en_ru_zh_2023_10_22_21-30-44'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_name = f't5_scratch_translate_en_ru_zh_'+time_in_second_to_textdate(current_time_in_second())\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a4c168a-4ea5-43b0-9f61-e490f13b650a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joefox/.pyvenv/base/lib/python3.9/site-packages/transformers/training_args.py:1389: FutureWarning: `--adafactor` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--optim adafactor` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "   f\"/checkpoints/{model_save_name}/ckpts\",\n",
    "   evaluation_strategy = \"steps\",\n",
    "   learning_rate=learning_rate,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   report_to='tensorboard',\n",
    "   logging_dir=f'/checkpoints/{model_save_name}/logs',\n",
    "   #weight_decay=0.01,\n",
    "   save_total_limit=save_total_limit,\n",
    "   #lr_scheduler_type = 'cosine', #['linear', 'cosine', \n",
    "   #'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt']\n",
    "   #lr_scheduler_type = 'constant_with_warmup',\n",
    "\n",
    "   metric_for_best_model='Chrf',\n",
    "   greater_is_better=True,\n",
    "   remove_unused_columns=False,\n",
    "   load_best_model_at_end=False,\n",
    "    \n",
    "   dataloader_num_workers=num_workers,\n",
    "\n",
    "   #warmup_steps=train_steps//20,\n",
    "\n",
    "   num_train_epochs=num_train_epochs,\n",
    "   #max_steps=train_steps,\n",
    "   eval_steps=eval_steps,\n",
    "   save_steps=save_steps,\n",
    "    \n",
    "   logging_steps=min(50,eval_steps,save_steps),\n",
    "   gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "   save_strategy=\"steps\",\n",
    "   tf32=True,\n",
    "   bf16=True,\n",
    "   fp16=False,\n",
    "   torch_compile=False,\n",
    "   #optim=\"adamw_torch_fused\",\n",
    "   adafactor=True,\n",
    "    \n",
    "   predict_with_generate=True,\n",
    "   generation_max_length=max_target_length,\n",
    "   #generation_config = model.generation_config,\n",
    "   #ignore_data_skip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35437ceb-b5d2-4618-8dd0-89ff22314b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from typing import Iterable, Tuple\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "            SequentialLR,\n",
    "            LinearLR,\n",
    "            CosineAnnealingLR,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57b7b25d-e011-4ee9-a983-a31ddaddf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWScale(Optimizer):\n",
    "    \"\"\"\n",
    "    This AdamW implementation is copied from Huggingface.\n",
    "    We modified it with Adagrad scaling by rms of a weight tensor\n",
    "\n",
    "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
    "    Regularization](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 1e-3):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to (0.9, 0.999)):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (`float`, *optional*, defaults to 1e-6):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
    "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
    "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rms(tensor):\n",
    "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                # /Adapt Step from Adafactor\n",
    "                step_size = step_size * max(1e-3, self._rms(p.data))\n",
    "                # /Adapt Step from Adafactor\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(p.data, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f70c2bfd-b9ce-40fe-b2f2-da2527f2665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm\", \"layernorm\", \"layer_norm\", \"ln\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": optim_weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamWScale(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85fc9ea6-4dd1-4ea1-a062-7c8bde917b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=0.5,\n",
    "        end_factor=1,\n",
    "        total_iters=warmup_steps,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "scheduler2 = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=train_steps - warmup_steps,\n",
    "    eta_min=learning_rate_final_cosine,\n",
    ")\n",
    "lr_scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[scheduler1, scheduler2],\n",
    "        milestones=[warmup_steps]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aa9fa80-a15a-4e43-87f2-df2cc72825e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollatorTransformers:\n",
    "    def __init__(self, pad_value=0, mask_pad_value=0, label_pad_value=-100):\n",
    "        self.pad_value = pad_value\n",
    "        self.mask_pad_value = mask_pad_value\n",
    "        self.label_pad_value = label_pad_value\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Специфика данного коллатора что он заточен именно под сетки marian\n",
    "        особенности в том, что pad_token = 1\n",
    "        а в метках паддинг заменяется последовательность -100\n",
    "        '''\n",
    "        #np_input, np_label, len_input_ids, len_labels = batch\n",
    "\n",
    "        #print(batch)\n",
    "        \n",
    "        #dict_batch = {k: [v] for k, v in batch.items()}\n",
    "        dict_batch = {k: [dic[k] for dic in batch] for k in batch[0].keys()}\n",
    "\n",
    "        batch_size = len(dict_batch['input_ids'])\n",
    "\n",
    "        # print(batch.keys())\n",
    "        # print(batch_size)\n",
    "        # print(batch)\n",
    "        dict_batch['input_ids'] = [torch.tensor(x.astype(np.int32)) for x in dict_batch['input_ids']]\n",
    "        dict_batch['labels'] = [torch.tensor(x.astype(np.int64)) for x in dict_batch['labels']]        \n",
    "        \n",
    "        dict_batch['attention_mask'] = [torch.ones_like(x) for x in dict_batch['input_ids']]\n",
    "        #print(batch['attention_mask'])\n",
    "\n",
    "        dict_batch.pop('len_input_ids')\n",
    "        dict_batch.pop('len_labels')\n",
    "\n",
    "        #batch['attention_mask'] = [torch.tensor(x) for x in batch['attention_mask']]\n",
    "        \n",
    "        dict_batch['input_ids'] = pad_sequence(dict_batch['input_ids'], batch_first=True, padding_value=self.pad_value)\n",
    "        dict_batch['attention_mask'] = pad_sequence(dict_batch['attention_mask'], batch_first=True, padding_value=self.mask_pad_value)\n",
    "        dict_batch['labels'] = pad_sequence(dict_batch['labels'], batch_first=True, padding_value=self.label_pad_value)\n",
    "\n",
    "        # print(batch.keys())\n",
    "        # print(batch['input_ids'].shape)\n",
    "        # print(batch['attention_mask'].shape)\n",
    "        # print(batch['labels'].shape)\n",
    "        # print(batch['input_ids'].dtype)\n",
    "        # print(batch['attention_mask'].dtype)\n",
    "        # print(batch['labels'].dtype)\n",
    "        \n",
    "        return dict_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "691ba6cd-f67d-4226-a9e1-df7dc9c5e385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = CollatorTransformers(tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id)\n",
    "#data_collator = CollatorTransformers(tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.eos_token_id)\n",
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afe4a2c6-f061-4086-9113-f11199024eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(rd_val,batch_size=batch_size,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cdc868c-5255-4884-924f-2ff1e784cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef0f468-f66a-4490-96a9-1543d940a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f105c69-bad1-4e51-a246-017b1aa9db32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_perplexity(pp_model, s):\n",
    "    if isinstance(s, str):\n",
    "        n = len(s.split())\n",
    "        sum_inv_logs = -1 * sum(score for score, _, _ in pp_model.full_scores(s))\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return math.pow(sum_inv_logs, 1.0/n)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb2a1f1d-4bd6-416e-a0aa-333961090d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(references, predictions):\n",
    "    '''\n",
    "    references, predictions\n",
    "    '''\n",
    "    #print('calculate rouge')\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in predictions]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in references]\n",
    "    \n",
    "    # print('predictions:',predictions[0])\n",
    "    # print('references:',references[0])\n",
    "\n",
    "    # print('decoded_preds:',decoded_preds[0])\n",
    "    # print('decoded_labels:',decoded_labels[0])\n",
    "    # print('-----------------')\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    #print('calculate bleu')\n",
    "    sacrebleu_result = sacrebleu_score.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    #print('calculate chrf')\n",
    "    chrf_result = metric_chrf.compute(predictions=predictions, references=references)\n",
    "    #result[\"chrf\"] = chrf_result[\"score\"]\n",
    "    result[\"eval_Chrf\"] = chrf_result[\"score\"]\n",
    "\n",
    "    list_ru_references = []\n",
    "    list_en_references = []\n",
    "    list_zh_references = []\n",
    "    list_ru_predictions = []\n",
    "    list_en_predictions = []\n",
    "    list_zh_predictions = []\n",
    "    for label, predict in zip(references,predictions):\n",
    "        lang = detect(label)\n",
    "        if  lang == 'ru':\n",
    "            list_ru_references.append(label)\n",
    "            list_ru_predictions.append(predict)\n",
    "        elif lang == 'en':\n",
    "            list_en_references.append(label)\n",
    "            list_en_predictions.append(predict)\n",
    "        else:\n",
    "            list_zh_references.append(label)\n",
    "            list_zh_predictions.append(predict)\n",
    "        \n",
    "    \n",
    "    #meteor_result = meteor_score.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    #result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result['sacrebleu'] = sacrebleu_result['score']\n",
    "    #result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "    \n",
    "    #print('calculate ppl')\n",
    "    list_ppl_ru = []\n",
    "    list_ppl = []\n",
    "    for predict in list_ru_predictions:\n",
    "        ppl = get_perplexity(pp_model_ru, predict)\n",
    "        list_ppl_ru.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_ru\"] = statistics.mean(list_ppl_ru)\n",
    "\n",
    "    list_ppl_en = []\n",
    "    for predict in list_en_predictions:\n",
    "        ppl = get_perplexity(pp_model_en, predict)\n",
    "        list_ppl_en.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_en\"] = statistics.mean(list_ppl_en)\n",
    "\n",
    "    list_ppl_zh = []\n",
    "    for predict in list_zh_predictions:\n",
    "        ppl = get_perplexity(pp_model_zh, predict)\n",
    "        list_ppl_zh.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_zh\"] = statistics.mean(list_ppl_zh)\n",
    "    result[\"ppl\"] = statistics.mean(list_ppl)\n",
    "\n",
    "    #print('calculate bertscore ru')\n",
    "    #results_bert_ru = bertscore_ru.compute(predictions=list_ru_predictions, references=list_ru_references, lang='ru', device='cpu')\n",
    "    #print('calculate bertscore en')\n",
    "    #results_bert_en = bertscore_en.compute(predictions=list_en_predictions, references=list_en_references, lang='en', device='cpu')\n",
    "    #print('calculate bertscore zh')\n",
    "    #results_bert_zh = bertscore_zh.compute(predictions=list_zh_predictions, references=list_zh_references, lang='zh', device='cpu')\n",
    "    # print(results_bert_ru['hashcode'])\n",
    "    # print(results_bert_en['hashcode'])\n",
    "    # print(results_bert_zh['hashcode'])\n",
    "    # for key in ['precision','recall','f1']:\n",
    "    #     result['bertscore_ru_' + key] = statistics.mean(results_bert_ru[key])\n",
    "    #     result['bertscore_en_' + key] = statistics.mean(results_bert_en[key])\n",
    "    #     result['bertscore_zh_' + key] = statistics.mean(results_bert_zh[key])\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe24d2de-d5dd-400e-8bd4-4a0b79dafbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_decode(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #print('=======================================')\n",
    "    # Decode generated summaries into text\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    #print('predictions',predictions)\n",
    "    #print('labels',labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return compute_metrics(decoded_labels, decoded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb8e85f-c3db-4331-bd55-f85ea5d2c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# accelerator = Accelerator(mixed_precision='fp16')\n",
    "# model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "205d3add-53a9-41ed-a081-29121158f22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.read_data_fast.FixLenFastReadDS at 0x7faa27065400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a74a47d7-b4fc-4d7e-910f-91a3a2ce051a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0fc1ce7-9b9b-4803-983d-21b701f5b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17593e5c-e1a6-4991-bf67-63095d926153",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, args, rd_train = accelerator.prepare(model, args, rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56e08336-a14e-46b2-b74b-9453de20369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a45509e8-90c1-4efe-a0b0-d73649aa6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Accelerator instance\n",
    "# accelerator = Accelerator(\n",
    "#     train_dataset=None,\n",
    "#     eval_dataset=None,\n",
    "#     compute_metrics=compute_metrics_decode,\n",
    "#     device=\"cuda\",\n",
    "#     acceleration_enabled=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c59c913d-9b87-488f-9038-d8ec30d0b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=rd_train,\n",
    "   eval_dataset=rd_val,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics_decode,\n",
    "   optimizers=(optimizer,lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52be45-2478-4a70-ba9e-33074245f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.amp.autocast(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1b175-3bcc-4bdb-9628-9bb705c3bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model- float32 train - bfloat16 - start 14700 and out of memory\n",
    "#model- float32 train - float16 - start 14700 and out of memory 46:23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9b6b2c-38e7-44b6-aa44-d98029a8b36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/13 00:39 < 01:08, 0.10 it/s, Epoch 0.43/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff8b26-8747-4d39-9074-5d0164c24944",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c0d81-9809-48c8-9815-6268aceb9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install micro_trainer_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d53a75-1dd7-4ead-9a74-fc8cd7b3c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_trainer_transformers import TrainigParameters, UniversalTrainingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31e80f-2dbb-483c-84b5-b9fd58df8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save_steps = args.eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4383e-13d4-4c9d-a20c-98928206484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_param = TrainigParameters()\n",
    "training_param.debug = False\n",
    "training_param.from_hf_training_arguments('t5_micro_transf', 't5_micro_transf_1',args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0933c-4611-418f-82f6-ea1c6e92e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666b27d-892d-44d5-860c-4643c86855ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UniversalTrainingModule(\n",
    "    model=model,\n",
    "    args=training_param,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=rd_train,\n",
    "    eval_dataset=rd_val,\n",
    "    optimizers=(optimizer,lr_scheduler),\n",
    "    compute_metrics=compute_metrics_decode    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9032ce-9cca-4460-8133-bc7906d88b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(resume_from_checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be3e24-64a4-4c5e-a4f5-8c96016551d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35766186-a033-407d-a0a2-ec8e5bbcb90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.train(resume_from_checkpoint='/checkpoints/t5_base_scratch_translate_en_ru_zh_2023_10_15_08-36-08/ckpts/checkpoint-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede050b4-5fe6-4875-b9a1-2a9f647c6f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.compute_metrics=compute_metrics_decode\n",
    "# eval_results = trainer.evaluate()\n",
    "# eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3d585-8f97-4bd9-8df2-0da8021eca92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = f'{root_dir}data/checkpoints/models/marian_en_ru_first4-finetuned/checkpoint-90000'\n",
    "# model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285bc98-422c-4267-978d-591ff479e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/data/models/' + model_save_name)\n",
    "tokenizer.save_pretrained('/data/models/' + model_save_name)\n",
    "#torch.save(model.state_dict(), '/data/models/' + model_save_name + \"/model.pt\") \n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9dfd7-2254-4157-bdf4-74d903c7dc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = T5ForConditionalGeneration.from_pretrained('/data/models/' + model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99552b4b-7c52-477c-94c6-3efbbdc271b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iter = iter(rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087ec41-750e-4b81-a34a-05750e9fac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_iter = iter(ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd5791-8b38-4f52-8d04-03d9a6e91e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(eval_iter)\n",
    "tokenizer.batch_decode(torch.tensor([data['input_ids'].astype(np.int32)]), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319d3a0-aeca-45b6-98af-c86fa7edd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48ffa7-c57a-4aee-b809-8d2ae1df6ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(torch.tensor([data['input_ids'].astype(np.int32)]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67d774-20ae-498f-8335-b3be6d1e5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32d496-b57c-49c7-bfec-47ae4c0be601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([data['input_ids'].astype(np.int32)]).to(model.device),max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199622ae-5fed-4db6-8cd7-ffc03c7a58b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode([data['labels'].astype(np.int32)], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b1ecd-a26d-48f4-8421-57fc85d06c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded_preds = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3da00-fe51-489f-8b44-76f7aca12017",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'translate to en: Привет как дела? У меня всё хорошо. А у тебя как дела? Я из Австрии. Давай пойдем вместе на улцу?'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832684fa-c8ff-42e5-ae6e-9f623df0c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer(text)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628f5df-e639-4546-b453-c160529a3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([b['input_ids']]).to(model.device),max_length=max_target_length)\n",
    "text2 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c9c37-83cc-4998-b22c-5700c7d2808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer('translate to ru: '+text2[0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dec1cc-c74f-4d4c-aae2-f926053d408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([b['input_ids']]).to(model.device),max_length=max_target_length)\n",
    "text3 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a676af-ae4c-4691-b332-6a05a7be1664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f10b5-5f65-432c-a127-de877f498d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_splitters = {'. ':'',' | ':'',' :: ':''}\n",
    "list_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09bb70-9b19-43cd-84a2-b68186974f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list_splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68d110-49ce-4df6-be96-76bd1ce5bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
