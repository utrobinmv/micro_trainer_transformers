{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d627b974-2c6f-4235-a136-ad698230a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41908918-ed7c-42c8-ac67-1da733b3a128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0463307d-c10c-4252-8f4d-216e02a29291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, T5Tokenizer, T5TokenizerFast, AutoModel, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdc511f-2f1c-457a-a6ba-3b5296d120cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import kenlm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea9a31e-17dd-4dcd-808a-a556a10ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6859c4-24b7-4e6d-ba0b-19c866af915b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#root_dir = '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf7fce1-399b-40a0-af29-32de84b7e77b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joefox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7860cbc5-87a0-4193-840e-23d4a6da23dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etap = 6 0.02\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "etap_train = 6\n",
    "\n",
    "if etap_train == 6:\n",
    "    batch_size = 16\n",
    "    gradient_accumulation_steps = 64\n",
    "    max_input_length = 200\n",
    "    max_target_length = 200\n",
    "\n",
    "    train_steps = 14187 #–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—Ç–æ—Ä–æ–º —à–∞–≥–µ\n",
    "    num_train_epochs = 1.1\n",
    "    eval_steps = 1000\n",
    "    save_steps = 1000\n",
    "    warmup_steps = 2000\n",
    "    save_total_limit = 10\n",
    "    learning_rate=2e-2\n",
    "    learning_rate_final_cosine=1e-5\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "optim_weight_decay = 0.0\n",
    "\n",
    "print('etap =',etap_train,learning_rate)\n",
    "\n",
    "streaming = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16409b00-f8c4-4390-88bf-ccab8a98f8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")\n",
    "sacrebleu_score = evaluate.load(\"sacrebleu\")\n",
    "#meteor_score = evaluate.load('meteor')\n",
    "bertscore_ru = evaluate.load(\"bertscore\")\n",
    "bertscore_en = evaluate.load(\"bertscore\")\n",
    "bertscore_zh = evaluate.load(\"bertscore\")\n",
    "metric_chrf = evaluate.load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc8824a-442d-4062-afa9-76d73b16076b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kenlm', 'models', 'tokenizers']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4388205-eacf-47ad-9097-d1f268900de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_model_ru = kenlm.Model(f'/data/kenlm/' + f'ru.arpa.bin')\n",
    "pp_model_en = kenlm.Model(f'/data/kenlm/' + f'en.arpa.bin')\n",
    "pp_model_zh = kenlm.Model(f'/data/kenlm/' + f'zh.arpa.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d1d43b-ecf3-48a5-bfd4-7893816c06a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_ru_zh_translate_corpus_freq_1  en_ru_zh_translate_corpus_micro\n"
     ]
    }
   ],
   "source": [
    "!ls /share/translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c19092-9fd1-46c9-9515-ba102f46c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.read_data_fast import FixLenFastReadDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17373862-1bc6-42e7-b5a8-48c330ef5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace__len__(a):\n",
    "#     return 32*30\n",
    "\n",
    "# FixLenFastReadDS.__len__ = replace__len__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768aff66-d1b3-4614-a0cd-5bd83d12d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/share/datasets_bin/en_ru_zh_translate_corpus_freq_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd59079e-8e16-43ee-8c9e-6961e7b76582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('/data/tokenizers/tokenizer_t5_en_ru_zh_65000', max_len=max_input_length)\n",
    "if etap_train == 6:\n",
    "    model_name = '/data/models/t5_scratch_translate_en_ru_zh_2023_10_16_20-36-18'\n",
    "    #model = T5ForConditionalGeneration.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688d782e-67e8-4cd4-8200-2951a98300d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d521b7-de7e-4ae4-8740-cb4fd41d6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if etap_train == 4:\n",
    "#     #–ó–∞–º–æ—Ä–æ–∑–∏–º –≤—Å–µ —Å–ª–æ–∏ –∫—Ä–æ–º–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False \n",
    "#     for param in model.shared.parameters():\n",
    "#         param.requires_grad = True \n",
    "#     for param in model.decoder.embed_tokens.parameters():\n",
    "#         param.requires_grad = True \n",
    "#     for param in model.encoder.embed_tokens.parameters():\n",
    "#         param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96464830-7007-4f2b-9216-d6db696308e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, "
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad,end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b13b4119-2e13-45c6-b243-8dfdc125a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d713bc69a2942d88bae2309269341d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fccdbbb2efb43c1bdc7a318f220a88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rd_val = FixLenFastReadDS(data_dir,'val',max_input_length,tokenizer)\n",
    "rd_train = FixLenFastReadDS(data_dir,'val',max_input_length,tokenizer)\n",
    "#rd_train = FixLenFastReadDS(data_dir,'train',max_input_length,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f06abc-08fe-4554-845a-97b543132959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11842, 11842)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rd_val), len(rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c4749fb-6f33-4893-8d1b-206934c31b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rd_val[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f298ae5-b971-495e-9053-762adeacd852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('translate to ru: I believe that is only in navigable waters? :: And like Tempels, Kagame believes that the ancient monument of Bantu natural language contains markers of ontological commitments that are shared among all speakers of these languages, including commitment to the reality of God.. The NDRC has promoted more than 1,200 projects to private investors, covering a wide range of areas including transportation, energy and environmental protection, with total investment surpassing 2.5 trillion yuan (about 360 billion U.S. dollars). :: In all kinds of analysis, all processes should be regarded as links in a continuous chain of events, based on the laws of classical physics (experimentally verified) that do not contradict each other.. It confers resistance to light, exposed to the sun, temperature changes and high resistance to daily use furniture.</s>',\n",
       " '–í –¥–æ—Å—Ç–∞—Ç–∫–µ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏—à—å –≤ –º–æ—Ä—Å–∫–∏—Ö –≤–æ–¥–∞—Ö. :: –ö–∞–∫ –∏ –¢–µ–º–ø–µ–ª—Å, –ö–∞–≥–∞–º–µ —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ –¥—Ä–µ–≤–Ω–∏–π –ø–∞–º—è—Ç–Ω–∏–∫ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –±–∞–Ω—Ç—É —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ä–∫–µ—Ä—ã –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç –≤—Å–µ –Ω–æ—Å–∏—Ç–µ–ª–∏ —ç—Ç–∏—Ö —è–∑—ã–∫–æ–≤, –≤ —Ç–æ–º —á–∏—Å–ª–µ –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –ë–æ–≥–∞.. –ö–æ–º–∏—Ç–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª —á–∞—Å—Ç–Ω—ã–º –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞–º –±–æ–ª–µ–µ 1200 –ø—Ä–æ–µ–∫—Ç–æ–≤ –≤ —Ç–∞–∫–∏—Ö —Å—Ñ–µ—Ä–∞—Ö, –∫–∞–∫ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –∏ –æ—Ö—Ä–∞–Ω–∞ —ç–∫–æ–ª–æ–≥–∏–∏, —Å –æ–±—â–∏–º –æ–±—ä–µ–º–æ–º –∫–∞–ø–∏—Ç–∞–ª–∞ –≤ —Ä–∞–∑–º–µ—Ä–µ —Å–≤—ã—à–µ 2,5 —Ç—Ä–ª–Ω —é–∞–Ω–µ–π /–æ–∫–æ–ª–æ 360 –º–ª—Ä–¥ –¥–æ–ª–ª–∞—Ä–æ–≤ –°–®–ê/. :: –í —Ö–æ–¥–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–æ–ª–∂–Ω—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å—Å—è –∫–∞–∫ –∑–≤–µ–Ω—å—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Å–æ–±—ã—Ç–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –∑–∞–∫–æ–Ω–∞—Ö –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π —Ñ–∏–∑–∏–∫–∏ (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö) –Ω–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—â–∏—Ö –¥—Ä—É–≥ –¥—Ä—É–≥—É.. –ü—Ä–∏–¥–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–≤–µ—Ç—É, –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—é —Å–æ–ª–Ω—Ü–∞, –∏–∑–º–µ–Ω–µ–Ω–∏—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, –≤—ã—Å–æ–∫—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–µ–±–µ–ª–∏.</s>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_input, np_label = rd_val[0]['input_ids'], rd_val[0]['labels']\n",
    "tokenizer.decode(np_input), tokenizer.decode(np_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5635ebdd-4b61-4f9a-9d4d-2417ef5ef9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ó–∞–º–æ—Ä–æ–∑–∏–º –≤—Å–µ —Å–ª–æ–∏ –∫—Ä–æ–º–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False \n",
    "# for param in model.shared.parameters():\n",
    "#     param.requires_grad = False \n",
    "# for param in model.decoder.embed_tokens.parameters():\n",
    "#     param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10e7c85f-5667-44b6-a9be-8777ded6d640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def current_time_in_second(): \n",
    "    #–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö\n",
    "    return round(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca872fc4-0eae-4f2e-a507-bcd8efbd44ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_in_second_to_textdate(time_in_second):  \n",
    "    #–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–µ—Ç —Å–µ–∫—É–Ω–¥—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—É—é –¥–∞—Ç—É\n",
    "    local_time = time.localtime(time_in_second)\n",
    "    str_time = time.strftime(\"%Y_%m_%d_%H-%M-%S\", local_time)\n",
    "    return str_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df40419a-8e31-4dfe-a32c-a2acaceadd51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5_scratch_translate_en_ru_zh_2023_10_22_21-30-44'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_name = f't5_scratch_translate_en_ru_zh_'+time_in_second_to_textdate(current_time_in_second())\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a4c168a-4ea5-43b0-9f61-e490f13b650a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joefox/.pyvenv/base/lib/python3.9/site-packages/transformers/training_args.py:1389: FutureWarning: `--adafactor` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--optim adafactor` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "   f\"/checkpoints/{model_save_name}/ckpts\",\n",
    "   evaluation_strategy = \"steps\",\n",
    "   learning_rate=learning_rate,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   report_to='tensorboard',\n",
    "   logging_dir=f'/checkpoints/{model_save_name}/logs',\n",
    "   #weight_decay=0.01,\n",
    "   save_total_limit=save_total_limit,\n",
    "   #lr_scheduler_type = 'cosine', #['linear', 'cosine', \n",
    "   #'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt']\n",
    "   #lr_scheduler_type = 'constant_with_warmup',\n",
    "\n",
    "   metric_for_best_model='Chrf',\n",
    "   greater_is_better=True,\n",
    "   remove_unused_columns=False,\n",
    "   load_best_model_at_end=False,\n",
    "    \n",
    "   dataloader_num_workers=num_workers,\n",
    "\n",
    "   #warmup_steps=train_steps//20,\n",
    "\n",
    "   num_train_epochs=num_train_epochs,\n",
    "   #max_steps=train_steps,\n",
    "   eval_steps=eval_steps,\n",
    "   save_steps=save_steps,\n",
    "    \n",
    "   logging_steps=min(50,eval_steps,save_steps),\n",
    "   gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "   save_strategy=\"steps\",\n",
    "   tf32=True,\n",
    "   bf16=True,\n",
    "   fp16=False,\n",
    "   torch_compile=False,\n",
    "   #optim=\"adamw_torch_fused\",\n",
    "   adafactor=True,\n",
    "    \n",
    "   predict_with_generate=True,\n",
    "   generation_max_length=max_target_length,\n",
    "   #generation_config = model.generation_config,\n",
    "   #ignore_data_skip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35437ceb-b5d2-4618-8dd0-89ff22314b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from typing import Iterable, Tuple\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim.lr_scheduler import (\n",
    "            SequentialLR,\n",
    "            LinearLR,\n",
    "            CosineAnnealingLR,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57b7b25d-e011-4ee9-a983-a31ddaddf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWScale(Optimizer):\n",
    "    \"\"\"\n",
    "    This AdamW implementation is copied from Huggingface.\n",
    "    We modified it with Adagrad scaling by rms of a weight tensor\n",
    "\n",
    "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
    "    Regularization](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 1e-3):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to (0.9, 0.999)):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (`float`, *optional*, defaults to 1e-6):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
    "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
    "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rms(tensor):\n",
    "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                # /Adapt Step from Adafactor\n",
    "                step_size = step_size * max(1e-3, self._rms(p.data))\n",
    "                # /Adapt Step from Adafactor\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(p.data, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f70c2bfd-b9ce-40fe-b2f2-da2527f2665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm\", \"layernorm\", \"layer_norm\", \"ln\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": optim_weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamWScale(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85fc9ea6-4dd1-4ea1-a062-7c8bde917b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=0.5,\n",
    "        end_factor=1,\n",
    "        total_iters=warmup_steps,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "scheduler2 = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=train_steps - warmup_steps,\n",
    "    eta_min=learning_rate_final_cosine,\n",
    ")\n",
    "lr_scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[scheduler1, scheduler2],\n",
    "        milestones=[warmup_steps]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aa9fa80-a15a-4e43-87f2-df2cc72825e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollatorTransformers:\n",
    "    def __init__(self, pad_value=0, mask_pad_value=0, label_pad_value=-100):\n",
    "        self.pad_value = pad_value\n",
    "        self.mask_pad_value = mask_pad_value\n",
    "        self.label_pad_value = label_pad_value\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ —á—Ç–æ –æ–Ω –∑–∞—Ç–æ—á–µ–Ω –∏–º–µ–Ω–Ω–æ –ø–æ–¥ —Å–µ—Ç–∫–∏ marian\n",
    "        –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç–æ–º, —á—Ç–æ pad_token = 1\n",
    "        –∞ –≤ –º–µ—Ç–∫–∞—Ö –ø–∞–¥–¥–∏–Ω–≥ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å -100\n",
    "        '''\n",
    "        #np_input, np_label, len_input_ids, len_labels = batch\n",
    "\n",
    "        #print(batch)\n",
    "        \n",
    "        #dict_batch = {k: [v] for k, v in batch.items()}\n",
    "        dict_batch = {k: [dic[k] for dic in batch] for k in batch[0].keys()}\n",
    "\n",
    "        batch_size = len(dict_batch['input_ids'])\n",
    "\n",
    "        # print(batch.keys())\n",
    "        # print(batch_size)\n",
    "        # print(batch)\n",
    "        dict_batch['input_ids'] = [torch.tensor(x.astype(np.int32)) for x in dict_batch['input_ids']]\n",
    "        dict_batch['labels'] = [torch.tensor(x.astype(np.int64)) for x in dict_batch['labels']]        \n",
    "        \n",
    "        dict_batch['attention_mask'] = [torch.ones_like(x) for x in dict_batch['input_ids']]\n",
    "        #print(batch['attention_mask'])\n",
    "\n",
    "        dict_batch.pop('len_input_ids')\n",
    "        dict_batch.pop('len_labels')\n",
    "\n",
    "        #batch['attention_mask'] = [torch.tensor(x) for x in batch['attention_mask']]\n",
    "        \n",
    "        dict_batch['input_ids'] = pad_sequence(dict_batch['input_ids'], batch_first=True, padding_value=self.pad_value)\n",
    "        dict_batch['attention_mask'] = pad_sequence(dict_batch['attention_mask'], batch_first=True, padding_value=self.mask_pad_value)\n",
    "        dict_batch['labels'] = pad_sequence(dict_batch['labels'], batch_first=True, padding_value=self.label_pad_value)\n",
    "\n",
    "        # print(batch.keys())\n",
    "        # print(batch['input_ids'].shape)\n",
    "        # print(batch['attention_mask'].shape)\n",
    "        # print(batch['labels'].shape)\n",
    "        # print(batch['input_ids'].dtype)\n",
    "        # print(batch['attention_mask'].dtype)\n",
    "        # print(batch['labels'].dtype)\n",
    "        \n",
    "        return dict_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "691ba6cd-f67d-4226-a9e1-df7dc9c5e385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = CollatorTransformers(tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id)\n",
    "#data_collator = CollatorTransformers(tokenizer.eos_token_id, tokenizer.pad_token_id, tokenizer.eos_token_id)\n",
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afe4a2c6-f061-4086-9113-f11199024eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(rd_val,batch_size=batch_size,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cdc868c-5255-4884-924f-2ff1e784cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef0f468-f66a-4490-96a9-1543d940a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f105c69-bad1-4e51-a246-017b1aa9db32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_perplexity(pp_model, s):\n",
    "    if isinstance(s, str):\n",
    "        n = len(s.split())\n",
    "        sum_inv_logs = -1 * sum(score for score, _, _ in pp_model.full_scores(s))\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return math.pow(sum_inv_logs, 1.0/n)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb2a1f1d-4bd6-416e-a0aa-333961090d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(references, predictions):\n",
    "    '''\n",
    "    references, predictions\n",
    "    '''\n",
    "    #print('calculate rouge')\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in predictions]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in references]\n",
    "    \n",
    "    # print('predictions:',predictions[0])\n",
    "    # print('references:',references[0])\n",
    "\n",
    "    # print('decoded_preds:',decoded_preds[0])\n",
    "    # print('decoded_labels:',decoded_labels[0])\n",
    "    # print('-----------------')\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    #print('calculate bleu')\n",
    "    sacrebleu_result = sacrebleu_score.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    #print('calculate chrf')\n",
    "    chrf_result = metric_chrf.compute(predictions=predictions, references=references)\n",
    "    #result[\"chrf\"] = chrf_result[\"score\"]\n",
    "    result[\"eval_Chrf\"] = chrf_result[\"score\"]\n",
    "\n",
    "    list_ru_references = []\n",
    "    list_en_references = []\n",
    "    list_zh_references = []\n",
    "    list_ru_predictions = []\n",
    "    list_en_predictions = []\n",
    "    list_zh_predictions = []\n",
    "    for label, predict in zip(references,predictions):\n",
    "        lang = detect(label)\n",
    "        if  lang == 'ru':\n",
    "            list_ru_references.append(label)\n",
    "            list_ru_predictions.append(predict)\n",
    "        elif lang == 'en':\n",
    "            list_en_references.append(label)\n",
    "            list_en_predictions.append(predict)\n",
    "        else:\n",
    "            list_zh_references.append(label)\n",
    "            list_zh_predictions.append(predict)\n",
    "        \n",
    "    \n",
    "    #meteor_result = meteor_score.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    #prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    #result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result['sacrebleu'] = sacrebleu_result['score']\n",
    "    #result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "    \n",
    "    #print('calculate ppl')\n",
    "    list_ppl_ru = []\n",
    "    list_ppl = []\n",
    "    for predict in list_ru_predictions:\n",
    "        ppl = get_perplexity(pp_model_ru, predict)\n",
    "        list_ppl_ru.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_ru\"] = statistics.mean(list_ppl_ru)\n",
    "\n",
    "    list_ppl_en = []\n",
    "    for predict in list_en_predictions:\n",
    "        ppl = get_perplexity(pp_model_en, predict)\n",
    "        list_ppl_en.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_en\"] = statistics.mean(list_ppl_en)\n",
    "\n",
    "    list_ppl_zh = []\n",
    "    for predict in list_zh_predictions:\n",
    "        ppl = get_perplexity(pp_model_zh, predict)\n",
    "        list_ppl_zh.append(ppl)\n",
    "        list_ppl.append(ppl)\n",
    "    result[\"ppl_zh\"] = statistics.mean(list_ppl_zh)\n",
    "    result[\"ppl\"] = statistics.mean(list_ppl)\n",
    "\n",
    "    #print('calculate bertscore ru')\n",
    "    #results_bert_ru = bertscore_ru.compute(predictions=list_ru_predictions, references=list_ru_references, lang='ru', device='cpu')\n",
    "    #print('calculate bertscore en')\n",
    "    #results_bert_en = bertscore_en.compute(predictions=list_en_predictions, references=list_en_references, lang='en', device='cpu')\n",
    "    #print('calculate bertscore zh')\n",
    "    #results_bert_zh = bertscore_zh.compute(predictions=list_zh_predictions, references=list_zh_references, lang='zh', device='cpu')\n",
    "    # print(results_bert_ru['hashcode'])\n",
    "    # print(results_bert_en['hashcode'])\n",
    "    # print(results_bert_zh['hashcode'])\n",
    "    # for key in ['precision','recall','f1']:\n",
    "    #     result['bertscore_ru_' + key] = statistics.mean(results_bert_ru[key])\n",
    "    #     result['bertscore_en_' + key] = statistics.mean(results_bert_en[key])\n",
    "    #     result['bertscore_zh_' + key] = statistics.mean(results_bert_zh[key])\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe24d2de-d5dd-400e-8bd4-4a0b79dafbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_decode(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #print('=======================================')\n",
    "    # Decode generated summaries into text\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    #print('predictions',predictions)\n",
    "    #print('labels',labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return compute_metrics(decoded_labels, decoded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb8e85f-c3db-4331-bd55-f85ea5d2c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# accelerator = Accelerator(mixed_precision='fp16')\n",
    "# model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "205d3add-53a9-41ed-a081-29121158f22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.read_data_fast.FixLenFastReadDS at 0x7faa27065400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a74a47d7-b4fc-4d7e-910f-91a3a2ce051a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0fc1ce7-9b9b-4803-983d-21b701f5b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17593e5c-e1a6-4991-bf67-63095d926153",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, args, rd_train = accelerator.prepare(model, args, rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56e08336-a14e-46b2-b74b-9453de20369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a45509e8-90c1-4efe-a0b0-d73649aa6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Accelerator instance\n",
    "# accelerator = Accelerator(\n",
    "#     train_dataset=None,\n",
    "#     eval_dataset=None,\n",
    "#     compute_metrics=compute_metrics_decode,\n",
    "#     device=\"cuda\",\n",
    "#     acceleration_enabled=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c59c913d-9b87-488f-9038-d8ec30d0b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=rd_train,\n",
    "   eval_dataset=rd_val,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics_decode,\n",
    "   optimizers=(optimizer,lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52be45-2478-4a70-ba9e-33074245f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.amp.autocast(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1b175-3bcc-4bdb-9628-9bb705c3bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model- float32 train - bfloat16 - start 14700 and out of memory\n",
    "#model- float32 train - float16 - start 14700 and out of memory 46:23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9b6b2c-38e7-44b6-aa44-d98029a8b36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/13 00:39 < 01:08, 0.10 it/s, Epoch 0.43/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyvenv/base/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff8b26-8747-4d39-9074-5d0164c24944",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c0d81-9809-48c8-9815-6268aceb9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install micro_trainer_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d53a75-1dd7-4ead-9a74-fc8cd7b3c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_trainer_transformers import TrainigParameters, UniversalTrainingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31e80f-2dbb-483c-84b5-b9fd58df8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save_steps = args.eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4383e-13d4-4c9d-a20c-98928206484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_param = TrainigParameters()\n",
    "training_param.debug = False\n",
    "training_param.from_hf_training_arguments('t5_micro_transf', 't5_micro_transf_1',args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0933c-4611-418f-82f6-ea1c6e92e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666b27d-892d-44d5-860c-4643c86855ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UniversalTrainingModule(\n",
    "    model=model,\n",
    "    args=training_param,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=rd_train,\n",
    "    eval_dataset=rd_val,\n",
    "    optimizers=(optimizer,lr_scheduler),\n",
    "    compute_metrics=compute_metrics_decode    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9032ce-9cca-4460-8133-bc7906d88b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(resume_from_checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be3e24-64a4-4c5e-a4f5-8c96016551d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35766186-a033-407d-a0a2-ec8e5bbcb90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.train(resume_from_checkpoint='/checkpoints/t5_base_scratch_translate_en_ru_zh_2023_10_15_08-36-08/ckpts/checkpoint-999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede050b4-5fe6-4875-b9a1-2a9f647c6f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.compute_metrics=compute_metrics_decode\n",
    "# eval_results = trainer.evaluate()\n",
    "# eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3d585-8f97-4bd9-8df2-0da8021eca92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = f'{root_dir}data/checkpoints/models/marian_en_ru_first4-finetuned/checkpoint-90000'\n",
    "# model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285bc98-422c-4267-978d-591ff479e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/data/models/' + model_save_name)\n",
    "tokenizer.save_pretrained('/data/models/' + model_save_name)\n",
    "#torch.save(model.state_dict(), '/data/models/' + model_save_name + \"/model.pt\") \n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9dfd7-2254-4157-bdf4-74d903c7dc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = T5ForConditionalGeneration.from_pretrained('/data/models/' + model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99552b4b-7c52-477c-94c6-3efbbdc271b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iter = iter(rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087ec41-750e-4b81-a34a-05750e9fac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_iter = iter(ds_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd5791-8b38-4f52-8d04-03d9a6e91e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(eval_iter)\n",
    "tokenizer.batch_decode(torch.tensor([data['input_ids'].astype(np.int32)]), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319d3a0-aeca-45b6-98af-c86fa7edd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48ffa7-c57a-4aee-b809-8d2ae1df6ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(torch.tensor([data['input_ids'].astype(np.int32)]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67d774-20ae-498f-8335-b3be6d1e5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32d496-b57c-49c7-bfec-47ae4c0be601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([data['input_ids'].astype(np.int32)]).to(model.device),max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199622ae-5fed-4db6-8cd7-ffc03c7a58b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode([data['labels'].astype(np.int32)], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b1ecd-a26d-48f4-8421-57fc85d06c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded_preds = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3da00-fe51-489f-8b44-76f7aca12017",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'translate to en: –ü—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞? –£ –º–µ–Ω—è –≤—Å—ë —Ö–æ—Ä–æ—à–æ. –ê —É —Ç–µ–±—è –∫–∞–∫ –¥–µ–ª–∞? –Ø –∏–∑ –ê–≤—Å—Ç—Ä–∏–∏. –î–∞–≤–∞–π –ø–æ–π–¥–µ–º –≤–º–µ—Å—Ç–µ –Ω–∞ —É–ª—Ü—É?'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832684fa-c8ff-42e5-ae6e-9f623df0c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer(text)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628f5df-e639-4546-b453-c160529a3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([b['input_ids']]).to(model.device),max_length=max_target_length)\n",
    "text2 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c9c37-83cc-4998-b22c-5700c7d2808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer('translate to ru: '+text2[0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dec1cc-c74f-4d4c-aae2-f926053d408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(torch.tensor([b['input_ids']]).to(model.device),max_length=max_target_length)\n",
    "text3 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a676af-ae4c-4691-b332-6a05a7be1664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f10b5-5f65-432c-a127-de877f498d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_splitters = {'. ':'',' | ':'',' :: ':''}\n",
    "list_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09bb70-9b19-43cd-84a2-b68186974f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list_splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68d110-49ce-4df6-be96-76bd1ce5bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
